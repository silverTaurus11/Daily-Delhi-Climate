# -*- coding: utf-8 -*-
"""DailyDelhiClimate.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QHKzWeqMCIY3yCsHaR6MKFEDSOnGFDqh
"""

import numpy as np
import pandas as pd
from keras.layers import Dense, LSTM
import matplotlib.pyplot as plt
import tensorflow as tf

#membaca data file csv
data_train = pd.read_csv('DailyDelhiClimateTrain.csv')
data_train.head()

data_train.isnull().sum()

#membuat grafik
date = data_train['date'].values
wind_speed = data_train['wind_speed'].values
     
plt.figure(figsize=(15,5))
plt.plot(date, wind_speed)
plt.title('Daily Delhi Climate', fontsize=20)

def windowed_dataset(series, window_size, batch_size, shuffle_buffer):
    series = tf.expand_dims(series, axis=-1)
    ds = tf.data.Dataset.from_tensor_slices(series)
    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)
    ds = ds.flat_map(lambda w: w.batch(window_size + 1))
    ds = ds.shuffle(shuffle_buffer)
    ds = ds.map(lambda w: (w[:-1], w[1:]))
    return ds.batch(batch_size).prefetch(1)

from sklearn.model_selection import train_test_split

#membagi data ke training data dan validation data
wind_speed_train, wind_speed_test = train_test_split(wind_speed, test_size=0.2)

print('Jumlah Total Data Wind Speed', len(wind_speed))
print('Jumlah Data Wind Speed Latihan', len(wind_speed_train))
print('Jumlah Data Wind Speed Tes', len(wind_speed_test))

#mengubah ke data set
wind_speed_set = windowed_dataset(wind_speed, window_size=60, batch_size=100, shuffle_buffer=1000)
wind_speed_train_set = windowed_dataset(wind_speed_train, window_size=60, batch_size=100, shuffle_buffer=1000)
wind_speed_test_set = windowed_dataset(wind_speed_test, window_size=60, batch_size=100, shuffle_buffer=1000)

#menghitung 10% dari skala data
maximum = max(wind_speed)
minimum = min(wind_speed)
maximum_mae = (maximum-minimum)*0.1
print(maximum_mae)

#membuat sistem callback
class myCallback(tf.keras.callbacks.Callback):
    def __init__(self, maximum_mae):
       self.maximum_mae = maximum_mae


    def on_epoch_end(self, epoch, logs={}):
        if(logs.get('mae') < self.maximum_mae):
          print("\MAE dari model < 10% skala data.")
          self.model.stop_training = True
callbacks = myCallback(maximum_mae)

#membuat model sequential
model = tf.keras.models.Sequential([
      tf.keras.layers.LSTM(48, return_sequences=True),
      tf.keras.layers.LSTM(48),
      tf.keras.layers.Dense(24, activation="relu"),
      tf.keras.layers.Dense(8, activation="relu"),
      tf.keras.layers.Dense(1),
])

#membuat optimizers
optimizer = tf.keras.optimizers.SGD(lr=1.0000e-04, momentum=0.9)
model.compile(loss=tf.keras.losses.Huber(),
              optimizer=optimizer,
              metrics=["mae"])
history = model.fit(wind_speed_train_set, validation_data=(wind_speed_test_set), verbose=2, epochs=200, callbacks=[callbacks])

# Commented out IPython magic to ensure Python compatibility.
# visualisasi hasil data training (Plot accuracy & loss model)

import numpy as np
from google.colab import files
from keras.preprocessing import image
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
# %matplotlib inline
 

mae = history.history['mae']
val_mae = history.history['val_mae']

loss = history.history['loss']
val_loss = history.history['val_loss']

plt.plot(mae, label='Training Mae')
plt.plot(val_mae, label='Validation Mae')
plt.title('Model mae')
plt.ylabel('Mae')
plt.xlabel('Epoch')
plt.legend(['Train'], loc='lower right')
plt.show()

plt.plot(loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train'], loc='upper right')
plt.show()